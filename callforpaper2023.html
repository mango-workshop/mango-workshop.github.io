<!DOCTYPE html>
<html>

  <!--
	Twenty by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->

<head>
    <title>E2EAD@CVPR 2023</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="Website for CVPR 2023 E2EAD Workshop">
    <meta name="keywords" content="" />

    <!-- CSS  --> . 
    <link rel="stylesheet" href="/css/skel.css">
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="/css/style-wide.css">
    <link rel="stylesheet" href="/css/style-noscript.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <!--[if lte IE 8]><script src="/css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/jquery.scrollgress.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>

    <meta name="viewport" content="width=device-width">

    <link rel="canonical" href="https://e2ead.github.io/">

</head>


  <body class="index">

    <header id="header">
    <h1 id="logo"><a href="/">E2EAD</a></h1>
    <nav id="nav">
        <ul>

            <li><a href="/2023.html"><span class="label">Home</span></a></li>
            <li><a href="/callforpaper2023.html"><span class="label">Call for Paper</span></a></li>


        </ul>
    </nav>
</header>

    <div class="page-content">
        <!-- Main -->
<article id="main">
  <!-- One -->
  <section class="wrapper style3 container special">
    <!-- Content -->
    <div class="content">
      <section>
        <header>
          <h2><strong>Overview</strong></h2>
        </header>
        <p>
            A diversity of computer vision capabilities are all critical in building industry-level autonomous driving systems, ranging from 2D to 3D perception, prediction, planning, to scene simulation. This has inspired a surge of relevant research and publications in leading venues such as CVPR, ICCV and ECCV, growing at a fast pace with increasingly accurate and efficient new methods (e.g. BEV-based 3D detection, HDMapNet, NeRF) developed continuously. Much more than simple combination of individual independently developed methods, autonomous driving also requires synergistic integration of different functions as a whole. This however is far away from the current situation that researchers in the sub-fields of perception, planning and simulation make largely limited idea exchange and communication. This calls for a system-level perspective on the advancement of autonomous driving, e.g. to establish end-to-end realtime AI systems streamlining multiple sensors (camera, LiDAR, Radar) and all the necessary functions/tasks (e.g. object detection, lane detection, HD map construction, path planning) under a single umbrella. To achieve this accomplishment, we are still faced with fundamental challenges such as (1) out-of-date visual perception and path planning methods, (2) overly simple integration strategies, (3) lack of high-quality simulation environments. In this context, this workshop aims to provide a platform where researchers from different sub-fields can focus on exchanging the frontier ideas across boundaries, leading to holistic system-aware understanding and systematic research attempts in the future.
        </p>
      </section>
    </div>
  </section>

  <section class="wrapper style3 container special">
    <!-- Content -->
    <div class="content">
      <section>
        <header>
          <h2><strong>Topics</strong></h2>
        </header>
        
        <p>
          <ul style="text-align: left;">
            <li>1. 3D obejct detetion</li>
            <li>2. Traffic lane detection and HD map construction</li>
            <li>3. End-to-end perception and path planning</li>
            <li>4. Autonomous driving environment simulation</li>
          </ul>
        </p>
        <p>
            Specifically, with Topic 1 and Topic 2, we encourage the researchers to illustrate the frontier of visual perception for autopilot. Whilst the researchers working on Topic 3 could share their thinking in path planning from the end-to-end perception viewpoint. The Topic 4 investigates the training data issue involved in all the previous topics by real-world rendering and simulation. Other topics related to end-to-end driving system design are all welcome and valuable to discuss.
        </p>
      </section>
    </div>
  </section>

  <section class="wrapper style3 container special">
    <!-- Content -->
    <div class="major">
      <section>
        <header>
          <h2><strong>Submission</strong></h2>
        </header>
      </section>
    </div>
    <div class="row">
  
      <div class="6u">
    
        <section>
            <header>
                <h3>Instructions</h3>
            </header>
            <p>Paper submission will follow <a href="https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip"> CVPR 2023</a> format. All papers will be reviewed with double blind policy. Sbumission is through CMT.</p>
            <p><a href="https://cmt3.research.microsoft.com/" target="_blank">CMT Submission</a> </p> 
        </section>
    </div>
    <div class="6u">
    
        <section>
            <header>
                <h3>Important Dates</h3>
            </header>
                <table >
                    <tbody >
                        <tr>
                            <td><strong>Action</strong></td>
                            <td><strong>Date</strong></td>
                        </tr>
                        <tr>
                            <td><span >Paper submission deadline</span></td>
                            <td><span ></span>Feb 20th, 2023</span></td>
                        </tr>
                        <tr>
                            <td><span >Notification to authors</span></td>
                            <td><span ></span>Feb 27th, 2023</span></td>
                        </tr>
                        <tr>
                            <td><span >Camera ready deadline</span></td>
                            <td><span ></span>April 8th, 2023</span></td>
                        </tr>
                    </tbody>
                </table>
        </section>      
    </div>
  </section>

</article>

    </div>
    <!-- Footer -->
<footer id="footer">

    <ul class="icons">
            
            
            
            
            
    </ul>

    <ul class="copyright">
        <li>&copy; Jiachen Lu 2022. Adapted from <a href="https://ngr-co3d.github.io/"> NGR-CO3D </a> and <a href="https://dynavis.github.io/">DynaVis</a>.</li>
    </ul>

</footer>


  </body>

</html>
